---

copyright:
  years: 2014, 2025
lastupdated: "2025-01-10"


keywords: oks, iro, openshift, red hat, red hat openshift

subcollection: openshift

---


{{site.data.keyword.attribute-definition-list}}





# Logging for clusters
{: #health}

For cluster and app logs, {{site.data.keyword.openshiftlong}} clusters include built-in tools to help you manage the health of your single cluster instance. You can also set up {{site.data.keyword.cloud_notm}} tools for multi-cluster analysis or other use cases, such as {{site.data.keyword.containerlong_notm}} cluster add-ons: {{site.data.keyword.la_full_notm}} and {{site.data.keyword.mon_full_notm}}.
{: shortdesc}

## Understanding options for logging
{: #oc_logmet_options}

To help understand when to use the built-in {{site.data.keyword.redhat_openshift_notm}} tools or {{site.data.keyword.cloud_notm}} integrations, review the following information.
{: shortdesc}

{{site.data.keyword.la_full_notm}}
:   Customizable user interface for live streaming of log tailing, real-time troubleshooting issue alerts, and log archiving.
    - Quick integration with the cluster via a script.
    - Aggregated logs across clusters and cloud providers.
    - Historical access to logs that is based on the plan you choose.
    - Highly available, scalable, and compliant with industry security standards.
    - Integrated with {{site.data.keyword.cloud_notm}} IAM for user access management.
    - Flexible plans, including a free `Lite` option.


Built-in {{site.data.keyword.redhat_openshift_notm}} logging tools
:   Built-in view of pod logs in the {{site.data.keyword.redhat_openshift_notm}} web console.
    - Built-in pod logs are not configured with persistent storage. You must integrate with a cloud database to back up the logging data and make it highly available, and manage the logs yourself.
  
    To set up an [OpenShift Container Platform Elasticsearch, Fluentd, and Kibana EFK stack](https://docs.openshift.com/container-platform/4.16/observability/logging/cluster-logging.html){: external}, see [installing the cluster logging operator](#oc_logging_operator). Keep in mind that your worker nodes must have at least 4 cores and GB memory to run the cluster logging stack.
    {: note}

Service logs: {{site.data.keyword.at_full}}
:   Use {{site.data.keyword.at_full_notm}} to view cluster management events that are generated by the {{site.data.keyword.openshiftlong_notm}} API. To access these logs, [provision an instance of {{site.data.keyword.at_full_notm}}](/docs/activity-tracker?topic=activity-tracker-getting-started). For more information about the types of {{site.data.keyword.containerlong_notm}} events that you can track, see [Activity Tracker events](/docs/openshift?topic=openshift-at_events_ref).

API server logs: {{site.data.keyword.la_full_notm}}
:   Customizable user interface for live streaming of log tailing, real-time troubleshooting issue alerts, and log archiving.
    - Quick integration with the cluster via a script.
    - Aggregated logs across clusters and cloud providers.
    - Historical access to logs that is based on the plan you choose.
    - Highly available, scalable, and compliant with industry security standards.
    - Integrated with {{site.data.keyword.cloud_notm}} IAM for user access management.
    - Flexible plans, including a free `Lite` option.
  
Built-in {{site.data.keyword.redhat_openshift_notm}} audit logging tools
:   API audit logging to monitor user-initiated activities is currently not supported.


## Forwarding cluster and app logs to {{site.data.keyword.la_full_notm}}
{: #openshift_logging}

The following steps are deprecated. The observability CLI plug-in `ibmcloud ob` and the `v2/observe` endpoints are deprecated and support ends on 28 March 2025. You can now manage your logging and monitoring integrations from the console or through the Helm charts. For the latest steps, see [Managing the Logging agent for Red Hat OpenShift on IBM Cloud clusters](/docs/cloud-logs?topic=cloud-logs-agent-openshift) or [Managing the Logging agent for IBM Cloud Kubernetes Service clusters](/docs/cloud-logs?topic=cloud-logs-agent-std-cluster)
{: deprecated}

Use the {{site.data.keyword.openshiftlong_notm}} observability plug-in to create a logging configuration for {{site.data.keyword.la_full_notm}} in your cluster, and use this logging configuration to automatically collect and forward pod logs to {{site.data.keyword.la_full_notm}}.
{: shortdesc}

Considerations for using the {{site.data.keyword.openshiftlong_notm}} observability plug-in:
* You can have only one logging configuration for {{site.data.keyword.la_full_notm}} in your cluster at a time. If you want to use a different {{site.data.keyword.la_full_notm}} service instance to send logs to, use the [`ibmcloud ob logging config replace`](/docs/containers?topic=containers-observability_cli#logging_config_replace) command.
* {{site.data.keyword.redhat_openshift_notm}} clusters in {{site.data.keyword.satelliteshort}} can't currently use the {{site.data.keyword.openshiftlong_notm}} console or the observability plug-in CLI to enable logging for {{site.data.keyword.satelliteshort}} clusters. You must manually deploy logging agents to your cluster to forward logs to {{site.data.keyword.la_short}}.
* If you created a {{site.data.keyword.la_short}} configuration in your cluster without using the {{site.data.keyword.openshiftlong_notm}} observability plug-in, you can use the [`ibmcloud ob logging agent discover`](/docs/containers?topic=containers-observability_cli#logging_agent_discover) command to make the configuration visible to the plug-in. Then, you can use the observability plug-in commands and functionality in the {{site.data.keyword.cloud_notm}} console to manage the configuration.

Before you begin
- Verify that you are assigned the **Editor** platform access role and **Manager** server access role for {{site.data.keyword.la_full_notm}}.
- Verify that you are assigned the **Administrator** platform access role and the **Manager** service access role for all Kubernetes namespaces in {{site.data.keyword.containerlong_notm}} to create the logging configuration. To view a logging configuration or launch the {{site.data.keyword.la_short}} dashboard after the logging configuration is created, users must be assigned the **Administrator** platform access role and the **Manager** service access for the `ibm-observe` Kubernetes namespace in {{site.data.keyword.containerlong_notm}}.
- If you want to use the CLI to set up the logging configuration:
    - [Install the {{site.data.keyword.openshiftlong_notm}} observability CLI plug-in (`ibmcloud ob`)](/docs/containers?topic=containers-cli-install).
    - [Access your {{site.data.keyword.redhat_openshift_notm}} cluster](/docs/openshift?topic=openshift-access_cluster).

To set up a logging configuration for your cluster,

1. Create an [{{site.data.keyword.la_full_notm}} service instance](/docs/log-analysis?topic=log-analysis-provision) and note the name of the instance. The service instance must belong to the same {{site.data.keyword.cloud_notm}} account where you created your cluster, but can be in a different resource group and {{site.data.keyword.cloud_notm}} region than your cluster.
2. Set up a logging configuration for your cluster. When you create the logging configuration, an {{site.data.keyword.redhat_openshift_notm}} project `ibm-observe` is created and a {{site.data.keyword.la_short}} agent is deployed as a daemon set to all worker nodes in your cluster. This agent collects logs with the extension `*.log` and extensionless files that are stored in the `/var/log` directory of your pod from all projects, including `kube-system`. The agent then forwards the logs to the {{site.data.keyword.la_full_notm}} service.

    - From the console
        1. From the [{{site.data.keyword.redhat_openshift_notm}} clusters console](https://cloud.ibm.com/kubernetes/clusters?platformType=openshift){: external}, select the cluster for which you want to create a {{site.data.keyword.la_short}} configuration.
        2. On the cluster **Overview** page, click **Connect**.
        3. Select the region and the {{site.data.keyword.la_full_notm}} service instance that you created earlier, and click **Connect**.

    - From the CLI
        1. Create the {{site.data.keyword.la_short}} configuration. When you create the {{site.data.keyword.la_short}} configuration, the ingestion key that was last added is retrieved automatically. If you want to use a different ingestion key, add the `--logdna-ingestion-key <ingestion_key>` option to the command.

            To use a different ingestion key after you created your logging configuration, use the [`ibmcloud ob logging config replace`](/docs/containers?topic=containers-observability_cli#logging_config_replace) command.
            {: tip}

            ```sh
            ibmcloud ob logging config create --cluster <cluster_name_or_ID> --instance <Log_Analysis_instance_name_or_ID>
            ```
            {: pre}

            Example output
            ```sh
            Creating configuration...
            OK
            ```
            {: screen}

        2. Verify that the logging configuration was added to your cluster.
            ```sh
            ibmcloud ob logging config list --cluster <cluster_name_or_ID>
            ```
            {: pre}

            Example output
            ```sh
            Listing configurations...

            OK
            Instance Name                Instance ID                            CRN   
            IBM Cloud Log Analysis-opm   1a111a1a-1111-11a1-a1aa-aaa11111a11a   crn:v1:prod:public:logdna:us-south:a/a11111a1aaaaa11a111aa11a1aa1111a:1a111a1a-1111-11a1-a1aa-aaa11111a11a::  
            ```
            {: screen}

3. Optional: Verify that the {{site.data.keyword.la_short}} agent was set up successfully.
    1. If you used the console to create the {{site.data.keyword.la_short}} configuration, log in to your cluster. For more information, see [Access your {{site.data.keyword.redhat_openshift_notm}} cluster](/docs/openshift?topic=openshift-access_cluster)..

    2. Verify that the daemon set for the {{site.data.keyword.la_short}} agent was created and all instances are listed as `AVAILABLE`.
        ```sh
        oc get daemonsets -n ibm-observe
        ```
        {: pre}

        Example output
        ```sh
        NAME           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
        logdna-agent   9         9         9       9            9           <none>          14m
        ```
        {: screen}

        The number of daemon set instances that are deployed equals the number of worker nodes in your cluster.

    3. Review the ConfigMap that was created for your {{site.data.keyword.la_short}} agent.
        ```sh
        oc describe configmap -n ibm-observe
        ```
        {: pre}

4. Access the logs for your pods from the {{site.data.keyword.la_short}} dashboard.
    1. From the [{{site.data.keyword.redhat_openshift_notm}} clusters console](https://cloud.ibm.com/kubernetes/clusters?platformType=openshift){: external}, select the cluster that you configured.  
    2. On the cluster **Overview** page, click **Launch**. The {{site.data.keyword.la_short}} dashboard opens.
    3. Review the pod logs that the {{site.data.keyword.la_short}} agent collected from your cluster. It might take a few minutes for your first logs to show.

5. Review how you can [search and filter logs in the {{site.data.keyword.la_short}} dashboard](/docs/log-analysis?topic=log-analysis-view_logs).


## Using the cluster logging operator
{: #oc_logging_operator}

To deploy the OpenShift Container Platform cluster logging operator and stack on your {{site.data.keyword.openshiftlong_notm}} cluster, see the [{{site.data.keyword.redhat_openshift_notm}} documentation](https://docs.openshift.com/container-platform/4.16/observability/logging/cluster-logging.html){: external}. Additionally, you must update the cluster logging instance to use an {{site.data.keyword.cloud_notm}} Block Storage storage class.
{: shortdesc}

1. Prepare your worker pool to run the operator.
    1. Create a [VPC](/docs/openshift?topic=openshift-add-workers-vpc) or [classic](/docs/openshift?topic=openshift-add-workers-classic) worker pool with a flavor of **at least 4 cores and 32 GB memory** and 3 worker nodes.
    2. [Label the worker pool](/docs/openshift?topic=openshift-worker-tag-label).
    3. [Taint the worker pool](/docs/openshift?topic=openshift-kubernetes-service-cli#worker_pool_taint) so that other workloads can't run on the worker pool.
2. [Access your {{site.data.keyword.redhat_openshift_notm}} cluster](/docs/openshift?topic=openshift-access_cluster).
3. From the {{site.data.keyword.redhat_openshift_notm}} web console **Administrator** perspective, click **Operators > Installed Operators**.
4. Click **Cluster Logging**.
5. In the **Provided APIs** section, **Cluster Logging** tile, click **Create Instance**.
6. Modify the configuration YAML to change the storage class for the ElasticSearch log storage from `gp2` to one of the following storage classes that vary with your cluster infrastructure provider.
    * **Classic clusters**: `ibmc-block-gold`
    * **VPC clusters**: `ibmc-vpc-block-10iops-tier`

    ```yaml
    ...
        elasticsearch:
          nodeCount: 3
          redundancyPolicy: SingleRedundancy
          storage:
            storageClassName: ibmc-block-gold #or ibmc-vpc-block-10iops-tier for VPC clusters
            size: 200G
    ...
    ```
    {: codeblock}
    
7. Modify the configuration YAML to include the node selector and toleration for the worker pool label and taint that you previously created. For more information and examples, see the following {{site.data.keyword.redhat_openshift_notm}} documents. The examples use a label and toleration of `logging: clo-efk`.
    * [Node selector](https://docs.openshift.com/container-platform/4.16/observability/logging/scheduling_resources/logging-node-selectors.html){: external}. Add the node selector to the Elasticsearch (`logstore`)and Kibana (`visualization`), and Fluentd (`collector.logs`) pods.
        ```yaml
        spec:
        logStore:
          elasticsearch:
            nodeSelector:
              logging: clo-efk
        ...
        visualization:
          kibana:
            nodeSelector:
              logging: clo-efk
        ...
        collection:
          logs:
            fluentd:
              nodeSelector:
                logging: clo-efk
        ```
        {: codeblock}

    * [Toleration](https://docs.openshift.com/container-platform/4.16/observability/logging/scheduling_resources/logging-taints-tolerations.html){: external}. Add the node selector to the Elasticsearch (`logstore`)and Kibana (`visualization`), and Fluentd (`collector.logs`) pods.
        ```yaml
        spec:
        logStore:
          elasticsearch:
            tolerations:
            - key: app
              value: clo-efk
              operator: "Exists"
              effect: "NoExecute"
        ...
        visualization:
          kibana:
            tolerations:
            - key: app
              value: clo-efk
              operator: "Exists"
              effect: "NoExecute"
        ...
        collection:
          logs:
            fluentd:
              tolerations:
              - key: app
                value: clo-efk
                operator: "Exists"
                effect: "NoExecute"
        ```
        {: codeblock}

8. Click **Create**.
9. Verify that the operator, Elasticsearch, Fluentd, and Kibana pods are all **Running**.
